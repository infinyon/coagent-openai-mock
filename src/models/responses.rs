use serde::Serialize;
use std::collections::HashMap;

/// Response from the completion API
#[derive(Debug, Serialize)]
pub struct CreateCompletionResponse {
    /// A unique identifier for the completion
    pub id: String,
    /// The object type, which is always "text_completion"
    pub object: String,
    /// The Unix timestamp (in seconds) of when the completion was created
    pub created: u64,
    /// The model used for the completion
    pub model: String,
    /// The list of completion choices
    pub choices: Vec<CompletionChoice>,
    /// Usage statistics for the completion request
    pub usage: CompletionUsage,
    /// This fingerprint represents the backend configuration that the model was run with
    #[serde(skip_serializing_if = "Option::is_none")]
    pub system_fingerprint: Option<String>,
}

/// A completion choice
#[derive(Debug, Serialize)]
pub struct CompletionChoice {
    /// The generated text
    pub text: String,
    /// The index of the choice in the list of choices
    pub index: u32,
    /// The log probabilities on the most likely tokens
    #[serde(skip_serializing_if = "Option::is_none")]
    pub logprobs: Option<CompletionLogprobs>,
    /// The reason the model stopped generating tokens
    pub finish_reason: String,
}

/// Log probabilities for completion tokens
#[derive(Debug, Serialize)]
pub struct CompletionLogprobs {
    /// The tokens chosen by the completion
    pub tokens: Vec<String>,
    /// The log probability of each token
    pub token_logprobs: Vec<Option<f64>>,
    /// The top log probabilities for each position
    #[serde(skip_serializing_if = "Option::is_none")]
    pub top_logprobs: Option<Vec<Option<HashMap<String, f64>>>>,
    /// The character offsets from the beginning of the returned text for each token
    pub text_offset: Vec<u32>,
}

/// Usage statistics for a completion request
#[derive(Debug, Serialize)]
pub struct CompletionUsage {
    /// Number of tokens in the prompt
    pub prompt_tokens: u32,
    /// Number of tokens in the generated completion
    pub completion_tokens: u32,
    /// Total number of tokens used in the request (prompt + completion)
    pub total_tokens: u32,
}

/// Response from the chat completion API
#[derive(Debug, Serialize)]
pub struct CreateChatCompletionResponse {
    /// A unique identifier for the chat completion
    pub id: String,
    /// The object type, which is always "chat.completion"
    pub object: String,
    /// The Unix timestamp (in seconds) of when the chat completion was created
    pub created: u64,
    /// The model used for the chat completion
    pub model: String,
    /// A list of chat completion choices
    pub choices: Vec<ChatCompletionChoice>,
    /// Usage statistics for the chat completion request
    pub usage: CompletionUsage,
    /// This fingerprint represents the backend configuration that the model was run with
    #[serde(skip_serializing_if = "Option::is_none")]
    pub system_fingerprint: Option<String>,
}

/// A chat completion choice
#[derive(Debug, Serialize)]
pub struct ChatCompletionChoice {
    /// The index of the choice in the list of choices
    pub index: u32,
    /// A chat completion message generated by the model
    pub message: ChatCompletionResponseMessage,
    /// The reason the model stopped generating tokens
    pub finish_reason: String,
    /// Log probability information for the choice
    #[serde(skip_serializing_if = "Option::is_none")]
    pub logprobs: Option<ChatCompletionLogprobs>,
}

/// A chat completion message in the response
#[derive(Debug, Serialize)]
pub struct ChatCompletionResponseMessage {
    /// The role of the author of this message
    pub role: String,
    /// The contents of the message
    #[serde(skip_serializing_if = "Option::is_none")]
    pub content: Option<String>,
    /// The tool calls generated by the model
    #[serde(skip_serializing_if = "Option::is_none")]
    pub tool_calls: Option<Vec<ChatCompletionMessageToolCall>>,
    /// The refusal message if the model refused to generate a response
    #[serde(skip_serializing_if = "Option::is_none")]
    pub refusal: Option<String>,
}

/// A tool call in a chat completion message
#[derive(Debug, Serialize)]
pub struct ChatCompletionMessageToolCall {
    /// The ID of the tool call
    pub id: String,
    /// The type of the tool. Currently, only `function` is supported
    #[serde(rename = "type")]
    pub tool_type: String,
    /// The function that the model called
    pub function: ChatCompletionFunctionCall,
}

/// A function call in a chat completion
#[derive(Debug, Serialize)]
pub struct ChatCompletionFunctionCall {
    /// The name of the function to call
    pub name: String,
    /// The arguments to call the function with, as generated by the model in JSON format
    pub arguments: String,
}

/// Log probability information for chat completion tokens
#[derive(Debug, Serialize)]
pub struct ChatCompletionLogprobs {
    /// A list of message content tokens with log probability information
    pub content: Vec<ChatCompletionTokenLogprob>,
}

/// Token log probability information for chat completions
#[derive(Debug, Serialize)]
pub struct ChatCompletionTokenLogprob {
    /// The token
    pub token: String,
    /// The log probability of this token
    pub logprob: f64,
    /// List of the most likely tokens and their log probabilities at this token position
    pub top_logprobs: Vec<ChatCompletionTopLogprob>,
    /// A list of integers representing the UTF-8 bytes representation of the token
    pub bytes: Option<Vec<u8>>,
}

/// Top log probability information for a token
#[derive(Debug, Serialize)]
pub struct ChatCompletionTopLogprob {
    /// The token
    pub token: String,
    /// The log probability of this token
    pub logprob: f64,
    /// A list of integers representing the UTF-8 bytes representation of the token
    pub bytes: Option<Vec<u8>>,
}

/// Response from the embeddings API
#[derive(Debug, Serialize)]
pub struct CreateEmbeddingResponse {
    /// The object type, which is always "list"
    pub object: String,
    /// The list of embedding objects
    pub data: Vec<EmbeddingData>,
    /// The model used for the embeddings
    pub model: String,
    /// Usage statistics for the embedding request
    pub usage: EmbeddingUsage,
}

/// An embedding object
#[derive(Debug, Serialize)]
pub struct EmbeddingData {
    /// The object type, which is always "embedding"
    pub object: String,
    /// The embedding vector
    pub embedding: Vec<f64>,
    /// The index of the embedding in the list of embeddings
    pub index: u32,
}

/// Usage statistics for an embedding request
#[derive(Debug, Serialize)]
pub struct EmbeddingUsage {
    /// Number of tokens in the input
    pub prompt_tokens: u32,
    /// Total number of tokens used in the request
    pub total_tokens: u32,
}

/// Error response structure that matches OpenAI API format
#[derive(Debug, Serialize)]
pub struct ErrorResponse {
    /// The error object
    pub error: ApiError,
}

/// API error details
#[derive(Debug, Serialize)]
pub struct ApiError {
    /// Error message
    pub message: String,
    /// Error type
    #[serde(rename = "type")]
    pub error_type: String,
    /// Error parameter (if applicable)
    #[serde(skip_serializing_if = "Option::is_none")]
    pub param: Option<String>,
    /// Error code (if applicable)
    #[serde(skip_serializing_if = "Option::is_none")]
    pub code: Option<String>,
}

impl CreateCompletionResponse {
    /// Create a new completion response
    pub fn new(
        id: String,
        model: String,
        created: u64,
        choices: Vec<CompletionChoice>,
        usage: CompletionUsage,
    ) -> Self {
        Self {
            id,
            object: "text_completion".to_string(),
            created,
            model,
            choices,
            usage,
            system_fingerprint: None,
        }
    }

    /// Set the system fingerprint
    pub fn with_system_fingerprint(mut self, fingerprint: String) -> Self {
        self.system_fingerprint = Some(fingerprint);
        self
    }
}

impl CompletionChoice {
    /// Create a new completion choice
    pub fn new(text: String, index: u32, finish_reason: String) -> Self {
        Self {
            text,
            index,
            logprobs: None,
            finish_reason,
        }
    }

    /// Set log probabilities for this choice
    pub fn with_logprobs(mut self, logprobs: CompletionLogprobs) -> Self {
        self.logprobs = Some(logprobs);
        self
    }
}

impl CompletionUsage {
    /// Create new completion usage statistics
    pub fn new(prompt_tokens: u32, completion_tokens: u32) -> Self {
        Self {
            prompt_tokens,
            completion_tokens,
            total_tokens: prompt_tokens + completion_tokens,
        }
    }
}

impl CreateChatCompletionResponse {
    /// Create a new chat completion response
    pub fn new(
        id: String,
        model: String,
        created: u64,
        choices: Vec<ChatCompletionChoice>,
        usage: CompletionUsage,
    ) -> Self {
        Self {
            id,
            object: "chat.completion".to_string(),
            created,
            model,
            choices,
            usage,
            system_fingerprint: None,
        }
    }

    /// Set the system fingerprint
    pub fn with_system_fingerprint(mut self, fingerprint: String) -> Self {
        self.system_fingerprint = Some(fingerprint);
        self
    }
}

impl ChatCompletionChoice {
    /// Create a new chat completion choice
    pub fn new(index: u32, message: ChatCompletionResponseMessage, finish_reason: String) -> Self {
        Self {
            index,
            message,
            finish_reason,
            logprobs: None,
        }
    }

    /// Set log probabilities for this choice
    pub fn with_logprobs(mut self, logprobs: ChatCompletionLogprobs) -> Self {
        self.logprobs = Some(logprobs);
        self
    }
}

impl ChatCompletionResponseMessage {
    /// Create a new assistant message with content
    pub fn assistant_message(content: String) -> Self {
        Self {
            role: "assistant".to_string(),
            content: Some(content),
            tool_calls: None,
            refusal: None,
        }
    }

    /// Create a new assistant message with tool calls
    pub fn assistant_message_with_tools(tool_calls: Vec<ChatCompletionMessageToolCall>) -> Self {
        Self {
            role: "assistant".to_string(),
            content: None,
            tool_calls: Some(tool_calls),
            refusal: None,
        }
    }

    /// Create a new assistant message with refusal
    pub fn assistant_refusal(refusal: String) -> Self {
        Self {
            role: "assistant".to_string(),
            content: None,
            tool_calls: None,
            refusal: Some(refusal),
        }
    }
}

impl ChatCompletionMessageToolCall {
    /// Create a new tool call
    pub fn new(id: String, function_name: String, arguments: String) -> Self {
        Self {
            id,
            tool_type: "function".to_string(),
            function: ChatCompletionFunctionCall {
                name: function_name,
                arguments,
            },
        }
    }
}

impl CreateEmbeddingResponse {
    /// Create a new embedding response
    pub fn new(data: Vec<EmbeddingData>, model: String, usage: EmbeddingUsage) -> Self {
        Self {
            object: "list".to_string(),
            data,
            model,
            usage,
        }
    }
}

impl EmbeddingData {
    /// Create a new embedding object
    pub fn new(embedding: Vec<f64>, index: u32) -> Self {
        Self {
            object: "embedding".to_string(),
            embedding,
            index,
        }
    }
}

impl EmbeddingUsage {
    /// Create new embedding usage statistics
    pub fn new(prompt_tokens: u32) -> Self {
        Self {
            prompt_tokens,
            total_tokens: prompt_tokens,
        }
    }
}

impl ErrorResponse {
    /// Create a new error response
    pub fn new(message: String, error_type: String) -> Self {
        Self {
            error: ApiError {
                message,
                error_type,
                param: None,
                code: None,
            },
        }
    }

    /// Create an invalid request error
    pub fn invalid_request_error(message: String) -> Self {
        Self::new(message, "invalid_request_error".to_string())
    }

    /// Create an authentication error
    pub fn authentication_error() -> Self {
        Self::new(
            "Incorrect API key provided".to_string(),
            "invalid_request_error".to_string(),
        )
    }

    /// Create a permission error
    pub fn permission_error() -> Self {
        Self::new(
            "You don't have access to this resource".to_string(),
            "insufficient_quota".to_string(),
        )
    }

    /// Set error parameter
    pub fn with_param(mut self, param: String) -> Self {
        self.error.param = Some(param);
        self
    }

    /// Set error code
    pub fn with_code(mut self, code: String) -> Self {
        self.error.code = Some(code);
        self
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_completion_response_serialization() {
        let usage = CompletionUsage::new(10, 20);
        let choice = CompletionChoice::new("Hello, world!".to_string(), 0, "stop".to_string());
        let response = CreateCompletionResponse::new(
            "cmpl-123".to_string(),
            "text-davinci-003".to_string(),
            1677649420,
            vec![choice],
            usage,
        );

        let json = serde_json::to_string(&response).expect("Failed to serialize");
        assert!(json.contains("\"object\":\"text_completion\""));
        assert!(json.contains("\"id\":\"cmpl-123\""));
        assert!(json.contains("\"model\":\"text-davinci-003\""));
        assert!(json.contains("\"total_tokens\":30"));
    }

    #[test]
    fn test_embedding_response_serialization() {
        let usage = EmbeddingUsage::new(8);
        let embedding = EmbeddingData::new(vec![0.1, 0.2, 0.3], 0);
        let response = CreateEmbeddingResponse::new(
            vec![embedding],
            "text-embedding-ada-002".to_string(),
            usage,
        );

        let json = serde_json::to_string(&response).expect("Failed to serialize");
        assert!(json.contains("\"object\":\"list\""));
        assert!(json.contains("\"model\":\"text-embedding-ada-002\""));
        assert!(json.contains("\"embedding\":[0.1,0.2,0.3]"));
    }

    #[test]
    fn test_error_response_serialization() {
        let error = ErrorResponse::invalid_request_error("Invalid model specified".to_string())
            .with_param("model".to_string());

        let json = serde_json::to_string(&error).expect("Failed to serialize");
        assert!(json.contains("\"type\":\"invalid_request_error\""));
        assert!(json.contains("\"message\":\"Invalid model specified\""));
        assert!(json.contains("\"param\":\"model\""));
    }

    #[test]
    fn test_completion_choice_with_logprobs() {
        let logprobs = CompletionLogprobs {
            tokens: vec!["Hello".to_string(), " world".to_string()],
            token_logprobs: vec![Some(-0.1), Some(-0.2)],
            top_logprobs: None,
            text_offset: vec![0, 5],
        };

        let choice = CompletionChoice::new("Hello world".to_string(), 0, "stop".to_string())
            .with_logprobs(logprobs);

        assert!(choice.logprobs.is_some());
        let logprobs = choice.logprobs.unwrap();
        assert_eq!(logprobs.tokens.len(), 2);
        assert_eq!(logprobs.token_logprobs.len(), 2);
    }

    #[test]
    fn test_usage_calculations() {
        let completion_usage = CompletionUsage::new(100, 50);
        assert_eq!(completion_usage.total_tokens, 150);
        assert_eq!(completion_usage.prompt_tokens, 100);
        assert_eq!(completion_usage.completion_tokens, 50);

        let embedding_usage = EmbeddingUsage::new(25);
        assert_eq!(embedding_usage.total_tokens, 25);
        assert_eq!(embedding_usage.prompt_tokens, 25);
    }
}
